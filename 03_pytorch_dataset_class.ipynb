{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self,data,targets) -> None:\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        current_sample = self.data[idx,:]\n",
    "        current_target = self.targets[idx]\n",
    "        return {\n",
    "            \"x\" : torch.tensor(current_sample,dtype=torch.float),\n",
    "            \"y\" : torch.tensor(current_target,dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mmake_classification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_informative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_redundant\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_repeated\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_clusters_per_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mflip_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mclass_sep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mhypercube\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshift\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Generate a random n-class classification problem.\n",
      "\n",
      "This initially creates clusters of points normally distributed (std=1)\n",
      "about vertices of an ``n_informative``-dimensional hypercube with sides of\n",
      "length ``2*class_sep`` and assigns an equal number of clusters to each\n",
      "class. It introduces interdependence between these features and adds\n",
      "various types of further noise to the data.\n",
      "\n",
      "Without shuffling, ``X`` horizontally stacks features in the following\n",
      "order: the primary ``n_informative`` features, followed by ``n_redundant``\n",
      "linear combinations of the informative features, followed by ``n_repeated``\n",
      "duplicates, drawn randomly with replacement from the informative and\n",
      "redundant features. The remaining features are filled with random noise.\n",
      "Thus, without shuffling, all useful features are contained in the columns\n",
      "``X[:, :n_informative + n_redundant + n_repeated]``.\n",
      "\n",
      "Read more in the :ref:`User Guide <sample_generators>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "n_samples : int, default=100\n",
      "    The number of samples.\n",
      "\n",
      "n_features : int, default=20\n",
      "    The total number of features. These comprise ``n_informative``\n",
      "    informative features, ``n_redundant`` redundant features,\n",
      "    ``n_repeated`` duplicated features and\n",
      "    ``n_features-n_informative-n_redundant-n_repeated`` useless features\n",
      "    drawn at random.\n",
      "\n",
      "n_informative : int, default=2\n",
      "    The number of informative features. Each class is composed of a number\n",
      "    of gaussian clusters each located around the vertices of a hypercube\n",
      "    in a subspace of dimension ``n_informative``. For each cluster,\n",
      "    informative features are drawn independently from  N(0, 1) and then\n",
      "    randomly linearly combined within each cluster in order to add\n",
      "    covariance. The clusters are then placed on the vertices of the\n",
      "    hypercube.\n",
      "\n",
      "n_redundant : int, default=2\n",
      "    The number of redundant features. These features are generated as\n",
      "    random linear combinations of the informative features.\n",
      "\n",
      "n_repeated : int, default=0\n",
      "    The number of duplicated features, drawn randomly from the informative\n",
      "    and the redundant features.\n",
      "\n",
      "n_classes : int, default=2\n",
      "    The number of classes (or labels) of the classification problem.\n",
      "\n",
      "n_clusters_per_class : int, default=2\n",
      "    The number of clusters per class.\n",
      "\n",
      "weights : array-like of shape (n_classes,) or (n_classes - 1,),              default=None\n",
      "    The proportions of samples assigned to each class. If None, then\n",
      "    classes are balanced. Note that if ``len(weights) == n_classes - 1``,\n",
      "    then the last class weight is automatically inferred.\n",
      "    More than ``n_samples`` samples may be returned if the sum of\n",
      "    ``weights`` exceeds 1. Note that the actual class proportions will\n",
      "    not exactly match ``weights`` when ``flip_y`` isn't 0.\n",
      "\n",
      "flip_y : float, default=0.01\n",
      "    The fraction of samples whose class is assigned randomly. Larger\n",
      "    values introduce noise in the labels and make the classification\n",
      "    task harder. Note that the default setting flip_y > 0 might lead\n",
      "    to less than ``n_classes`` in y in some cases.\n",
      "\n",
      "class_sep : float, default=1.0\n",
      "    The factor multiplying the hypercube size.  Larger values spread\n",
      "    out the clusters/classes and make the classification task easier.\n",
      "\n",
      "hypercube : bool, default=True\n",
      "    If True, the clusters are put on the vertices of a hypercube. If\n",
      "    False, the clusters are put on the vertices of a random polytope.\n",
      "\n",
      "shift : float, ndarray of shape (n_features,) or None, default=0.0\n",
      "    Shift features by the specified value. If None, then features\n",
      "    are shifted by a random value drawn in [-class_sep, class_sep].\n",
      "\n",
      "scale : float, ndarray of shape (n_features,) or None, default=1.0\n",
      "    Multiply features by the specified value. If None, then features\n",
      "    are scaled by a random value drawn in [1, 100]. Note that scaling\n",
      "    happens after shifting.\n",
      "\n",
      "shuffle : bool, default=True\n",
      "    Shuffle the samples and the features.\n",
      "\n",
      "random_state : int, RandomState instance or None, default=None\n",
      "    Determines random number generation for dataset creation. Pass an int\n",
      "    for reproducible output across multiple function calls.\n",
      "    See :term:`Glossary <random_state>`.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "X : ndarray of shape (n_samples, n_features)\n",
      "    The generated samples.\n",
      "\n",
      "y : ndarray of shape (n_samples,)\n",
      "    The integer labels for class membership of each sample.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "make_blobs : Simplified variant.\n",
      "make_multilabel_classification : Unrelated generator for multilabel tasks.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The algorithm is adapted from Guyon [1] and was designed to generate\n",
      "the \"Madelon\" dataset.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] I. Guyon, \"Design of experiments for the NIPS 2003 variable\n",
      "       selection benchmark\", 2003.\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\nikhi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\sklearn\\datasets\\_samples_generator.py\n",
      "\u001b[1;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "?make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets =make_classification(n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.02065646,  1.65384941, -0.18138796, ...,  1.50282996,\n",
       "        -1.53542364,  0.66430588],\n",
       "       [ 0.07620721, -0.59707232,  0.19369063, ...,  0.12063766,\n",
       "        -0.65470849, -0.01294099],\n",
       "       [ 1.28353091, -0.33901082, -0.78440635, ...,  1.09625622,\n",
       "        -0.8183609 , -2.11405585],\n",
       "       ...,\n",
       "       [ 1.0520145 ,  0.35559312,  0.8534902 , ..., -1.01370551,\n",
       "         0.17037841, -1.57854988],\n",
       "       [ 1.86496461, -0.96958355, -0.07415345, ..., -2.20172563,\n",
       "         0.62228015, -1.22757871],\n",
       "       [-1.14236786, -1.32328117,  0.28389334, ...,  0.59778148,\n",
       "         0.21565912,  0.76213613]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomDataset(data=data,targets=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomDataset at 0x1fe5dde9720>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(custom_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([-1.0207e+00,  1.6538e+00, -1.8139e-01, -1.7653e-04, -1.6791e+00,\n",
       "          6.2405e-01,  4.2041e-01,  4.1666e-01,  1.6087e+00,  3.4823e-01,\n",
       "          8.1052e-01, -1.5132e+00, -1.0388e+00, -4.5235e-01, -2.4448e+00,\n",
       "         -2.8233e-01, -9.1336e-01,  1.5028e+00, -1.5354e+00,  6.6431e-01]),\n",
       " 'y': tensor(1)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_dataset[0]['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_dataset[0]['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([ 2.7755,  2.0723,  1.7619, -0.1787,  0.2964, -0.0480,  1.1330, -1.5808,\n",
       "          0.5621, -1.2134, -1.4781,  0.5397, -1.6280, -0.7867,  2.3421,  0.4358,\n",
       "          0.9111, -0.5942, -0.0689, -2.1297]),\n",
       " 'y': tensor(0)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_dataset[345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([-1.0207e+00,  1.6538e+00, -1.8139e-01, -1.7653e-04, -1.6791e+00,\n",
      "         6.2405e-01,  4.2041e-01,  4.1666e-01,  1.6087e+00,  3.4823e-01,\n",
      "         8.1052e-01, -1.5132e+00, -1.0388e+00, -4.5235e-01, -2.4448e+00,\n",
      "        -2.8233e-01, -9.1336e-01,  1.5028e+00, -1.5354e+00,  6.6431e-01]), 'y': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(custom_dataset)):\n",
    "    print(custom_dataset[idx])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49925a85a85dac8a2dca5afbebde9d93135fdd4b812dbf89ef8044a5eedbb9bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
